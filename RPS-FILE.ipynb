{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafdaf4c-ec7a-40fb-b441-d1fb9128a25b",
   "metadata": {},
   "source": [
    "✊✋🤚 Hand Gesture Controlled Rock-Paper-Scissors Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0e2a22-4b6f-4a1d-b827-8e1fca9e377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in 'paper': 907\n",
      "Number of images in 'rock': 907\n",
      "Number of images in 'scissors': 903\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the dataset folder\n",
    "dataset_folder = 'Dataset-rps'  \n",
    "\n",
    "# Define the subfolders\n",
    "folders = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# Loop through each folder and count the number of files\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(dataset_folder, folder)  \n",
    "    # List all files in the folder and count them\n",
    "    files = os.listdir(folder_path)\n",
    "    image_count = len([f for f in files if f.endswith(('jpg', 'png', 'jpeg'))])  \n",
    "    print(f\"Number of images in '{folder}': {image_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1aaa455-463c-41ea-bb48-585e6a55b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2175 images belonging to 3 classes.\n",
      "Found 542 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda_\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 6s/step - accuracy: 0.4946 - loss: 1.0031 - val_accuracy: 0.8867 - val_loss: 0.5218\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/67\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:23\u001b[0m 5s/step - accuracy: 0.8750 - loss: 0.5122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda_\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.5122 - val_accuracy: 0.8770 - val_loss: 0.5206\n",
      "Epoch 3/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 7s/step - accuracy: 0.8092 - loss: 0.5528 - val_accuracy: 0.8125 - val_loss: 0.4378\n",
      "Epoch 4/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.5689 - val_accuracy: 0.8301 - val_loss: 0.4370\n",
      "Epoch 5/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 7s/step - accuracy: 0.8474 - loss: 0.4255 - val_accuracy: 0.9121 - val_loss: 0.2811\n",
      "Epoch 6/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9688 - loss: 0.1755 - val_accuracy: 0.9102 - val_loss: 0.2622\n",
      "Epoch 7/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 6s/step - accuracy: 0.8802 - loss: 0.3355 - val_accuracy: 0.9238 - val_loss: 0.2475\n",
      "Epoch 8/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.2598 - val_accuracy: 0.9219 - val_loss: 0.2477\n",
      "Epoch 9/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 8s/step - accuracy: 0.9018 - loss: 0.2808 - val_accuracy: 0.9492 - val_loss: 0.1887\n",
      "Epoch 10/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.8438 - loss: 0.3851 - val_accuracy: 0.9473 - val_loss: 0.1812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Paths to your dataset\n",
    "dataset_folder = 'Dataset-rps'\n",
    "\n",
    "# Initialize ImageDataGenerator with data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    rotation_range=20,  \n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2,  \n",
    "    zoom_range=0.2,  \n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest',  \n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Specify that this is for training\n",
    ")\n",
    "\n",
    "# Validation generator (for test data)\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Specify that this is for validation/testing\n",
    ")\n",
    "\n",
    "# Load pre-trained VGG16 model (without the top fully connected layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers for our rock-paper-scissors classification\n",
    "model = models.Sequential([\n",
    "    base_model,  # Add the base VGG16 model\n",
    "    layers.GlobalAveragePooling2D(),  \n",
    "    layers.Dense(512, activation='relu'),  # Fully connected layer\n",
    "    layers.Dropout(0.5),  \n",
    "    layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "history = model.fit(\n",
    "    train_generator,  # Train using the train generator\n",
    "    epochs=10,  \n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # how many bathes per epoch\n",
    "    validation_data=validation_generator,  # Use the validation generator for testing\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,  # Validation steps\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('rps_model.h5')  # Save the trained model for later use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f982f270-f614-466b-8088-873ba54e7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('rps_model.h5')\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f7a2d7-aa09-47d4-928d-610825b34a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import time\n",
    "\n",
    "# Load CNN model for Paper and Scissors\n",
    "model = load_model('rps_model.h5', compile=False)\n",
    "cnn_labels = ['Paper', 'Scissors']  # Only Paper and Scissors for CNN\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "\n",
    "# Check if fingers are folded (for Rock)\n",
    "def is_rock(hand_landmarks):\n",
    "    tips = [8, 12, 16, 20]  # Index, Middle, Ring, Pinky fingertips\n",
    "    folded = 0\n",
    "    for tip in tips:\n",
    "        if hand_landmarks.landmark[tip].y > hand_landmarks.landmark[tip - 2].y:\n",
    "            folded += 1\n",
    "    return folded == 4  # All fingers folded\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "warmup_time = 5  # 5 seconds initially\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Flip frame horizontally for natural mirror effect\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB for MediaPipe\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    prediction_text = \"No Hand Detected\"\n",
    "\n",
    "    # Check if hand landmarks are detected\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handLms in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if is_rock(handLms):  # Detect Rock using hand landmarks\n",
    "                prediction_text = \"Rock\"\n",
    "            else:\n",
    "                # Use CNN for Paper and Scissors\n",
    "                roi = cv2.resize(frame, (224, 224))  # Resize the frame to match input size of the model\n",
    "                roi = roi.astype('float32') / 255.0  # Normalize the image\n",
    "                roi = np.expand_dims(roi, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Get predictions from the CNN model\n",
    "                prediction = model.predict(roi)\n",
    "                class_index = np.argmax(prediction)  # Get index of the class with highest probability\n",
    "\n",
    "                if class_index == 0:  # Paper predicted\n",
    "                    prediction_text = \"Paper\"\n",
    "                else:  # If it's not Paper, then it must be Scissors\n",
    "                    prediction_text = \"Scissors\"\n",
    "\n",
    "    # Display the prediction\n",
    "    cv2.putText(frame, f\"Prediction: {prediction_text}\", (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with landmarks and prediction text\n",
    "    cv2.imshow(\"Rock-Paper-Scissors\", frame)\n",
    "\n",
    "    # Exit on ESC key\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    # After the first run, reduce the warmup time\n",
    "    if time.time() - start_time > warmup_time:\n",
    "        warmup_time = 2\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb80484f-dd26-4efe-9e85-9a1221a4976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load CNN model for Paper and Scissors\n",
    "model = load_model('rps_model.h5', compile=False)\n",
    "cnn_labels = ['Paper', 'Scissors']  # Only Paper and Scissors for CNN\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "\n",
    "# Check if fingers are folded (for Rock)\n",
    "def is_rock(hand_landmarks):\n",
    "    tips = [8, 12, 16, 20]  # Index, Middle, Ring, Pinky fingertips\n",
    "    folded = 0\n",
    "    for tip in tips:\n",
    "        if hand_landmarks.landmark[tip].y > hand_landmarks.landmark[tip - 2].y:\n",
    "            folded += 1\n",
    "    return folded == 4  # All fingers folded\n",
    "\n",
    "# Computer's choice logic\n",
    "def computer_choice():\n",
    "    return random.choice(['Rock', 'Paper', 'Scissors'])\n",
    "\n",
    "# Game result logic\n",
    "def get_winner(player, computer):\n",
    "    if player == computer:\n",
    "        return 'Draw'\n",
    "    elif (player == 'Rock' and computer == 'Scissors') or \\\n",
    "         (player == 'Paper' and computer == 'Rock') or \\\n",
    "         (player == 'Scissors' and computer == 'Paper'):\n",
    "        return 'Player'\n",
    "    else:\n",
    "        return 'Computer'\n",
    "\n",
    "# Initialize scores\n",
    "player_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)  # Set width to 1280\n",
    "cap.set(4, 720)   # Set height to 720\n",
    "start_time = time.time()\n",
    "warmup_time = 5  # Initial warmup time before the game starts\n",
    "\n",
    "while True:\n",
    "    # Countdown before the game starts (2 seconds)\n",
    "    for i in range(2, 0, -1):\n",
    "        frame = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "        cv2.putText(frame, f\"Get Ready! {i}\", (500, 360), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 0), 4)\n",
    "        cv2.imshow(\"Rock-Paper-Scissors Game\", frame)\n",
    "        if cv2.waitKey(1000) & 0xFF == 27:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit()\n",
    "\n",
    "    # Start round timer\n",
    "    round_start = time.time()\n",
    "    player_choice = \"None\"\n",
    "    prediction_text = \"Detecting...\"\n",
    "\n",
    "    while time.time() - round_start < 2:  # 2-second round timer\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)  # Flip frame horizontally for natural mirror effect\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB for MediaPipe\n",
    "        result = hands.process(img_rgb)\n",
    "\n",
    "        # Check for hand landmarks\n",
    "        if result.multi_hand_landmarks:\n",
    "            for handLms in result.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if is_rock(handLms):  # Detect Rock using hand landmarks\n",
    "                    prediction_text = \"Rock\"\n",
    "                else:\n",
    "                    # Use CNN for Paper and Scissors\n",
    "                    roi = cv2.resize(frame, (224, 224))  # Resize the frame to match input size of the model\n",
    "                    roi = roi.astype('float32') / 255.0  # Normalize the image\n",
    "                    roi = np.expand_dims(roi, axis=0)  # Add batch dimension\n",
    "\n",
    "                    # Get predictions from the CNN model\n",
    "                    prediction = model.predict(roi)\n",
    "                    class_index = np.argmax(prediction)  # Get index of the class with highest probability\n",
    "\n",
    "                    if class_index == 0:  # Paper predicted\n",
    "                        prediction_text = \"Paper\"\n",
    "                    else:  # If it's not Paper, then it must be Scissors\n",
    "                        prediction_text = \"Scissors\"\n",
    "\n",
    "        # Display the round countdown and prediction\n",
    "        cv2.putText(frame, f\"Detecting... {prediction_text}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        remaining_time = 2 - int(time.time() - round_start)\n",
    "        # Move \"Time Left\" text to a more left position\n",
    "        cv2.putText(frame, f\"Time Left: {remaining_time}s\", (900, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.imshow(\"Rock-Paper-Scissors Game\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit()\n",
    "\n",
    "    # Get computer's choice and calculate winner\n",
    "    computer_gesture = computer_choice()\n",
    "    winner = get_winner(prediction_text, computer_gesture)\n",
    "    \n",
    "    if winner == 'Player':\n",
    "        player_score += 1\n",
    "    elif winner == 'Computer':\n",
    "        computer_score += 1\n",
    "\n",
    "    # Show round result\n",
    "    result_frame = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "    cv2.putText(result_frame, f\"You: {prediction_text}\", (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 3)\n",
    "    cv2.putText(result_frame, f\"Computer: {computer_gesture}\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 255), 3)\n",
    "    \n",
    "    # Use if-elif-else to handle the winner message\n",
    "    if winner == 'Player':\n",
    "        cv2.putText(result_frame, f\"Winner: You\", (50, 280), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)\n",
    "    elif winner == 'Computer':\n",
    "        cv2.putText(result_frame, f\"Winner: Computer\", (50, 280), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "    else:\n",
    "        cv2.putText(result_frame, f\"It's a Draw!\", (50, 280), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 0), 4)\n",
    "    \n",
    "    cv2.putText(result_frame, f\"Score - You: {player_score} | Computer: {computer_score}\", (20, 360),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Game\", result_frame)\n",
    "    cv2.waitKey(3000)\n",
    "\n",
    "\n",
    "    # Check if anyone reached 5 points\n",
    "    if player_score == 5 or computer_score == 5:\n",
    "        final_frame = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "        winner_text = \"You Win!\" if player_score == 5 else \"Computer Wins!\"\n",
    "        cv2.putText(final_frame, winner_text, (500, 250), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 255), 5)\n",
    "        cv2.putText(final_frame, \"Press ESC to exit\", (400, 320), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "        cv2.imshow(\"Rock-Paper-Scissors Game\", final_frame)\n",
    "        if cv2.waitKey(0) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4e9fb-9904-46de-924c-e21aed4866a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
